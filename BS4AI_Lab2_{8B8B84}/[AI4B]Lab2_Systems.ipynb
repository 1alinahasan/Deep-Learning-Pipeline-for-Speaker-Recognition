{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d88b54",
   "metadata": {},
   "source": [
    "# Lab 2 - X-vector systems\n",
    "In this second lab, you will code, train, and optimize a x-vector system. At the end of this lab, you should be able to train your system on a given dataset, and have python files to re-use in the next labs of the semester, while using very simple metrics for the evaluation.\n",
    "\n",
    "*This lab is due for Sunday 15th, 11:59PM.*\n",
    "\n",
    "*If you have any question, please ask them during the lab session, email all TAs and instructors, or come during any of the 4 office hours.*\n",
    "\n",
    "You will need the ```dataset.py``` file from Lab 1 and the dataset used in Lab 1\n",
    "\n",
    "### Question 1 (10pts): Dataloaders\n",
    "Using and modifying the previous lab ```dataset.py``` file, load the train, val and test loaders. \n",
    "\n",
    "You will need a batch_size of 32 for the train and val, batch_size of 1 for the test.\n",
    "\n",
    "we want a 95/05 split between the train_val and the test speakers, and a 90/10 split between the train and the val utterances.\n",
    "\n",
    "You can optionnaly use a sampler to balance for the gender, or shuffle the train instead.\n",
    "\n",
    "You will need a collator that produces filterbanks, with 24 dimensions, a frame-length of 25ms.\n",
    "\n",
    "Add a print line that show how many different speakers are in the train+val set, how many are in the train, the val and the test set, and in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021d547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers | train+val: 1039 | train: 1039 | val: 965 | test: 55 | total: 1094\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_all_data\n",
    "test_loader, val_loader, train_loader = load_all_data(\n",
    "                                                metadata_file='VoxCeleb2_AE/metadata_dev.csv', \n",
    "                                                data_directory='VoxCeleb2_AE/dev', \n",
    "                                                batch_size=32,\n",
    "                                                train_val_prop=0.9,\n",
    "                                                train_test_prop=0.95,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e67bf5",
   "metadata": {},
   "source": [
    "### Question 2: The Classic X-vector Network\n",
    "\n",
    "In this question, you will implement the architecture of a **classic X-vector system** following the structure described in: [X-Vectors: Robust DNN Embeddings for Speaker Recognition (Povey et al., ICASSP 2018)](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf)\n",
    "\n",
    "Your implementation should follow the same logical organization as in the paper, consisting of:\n",
    "\n",
    "- **Frame-level TDNN (Time-Delay Neural Network) layers**  \n",
    "- **Statistics pooling** (mean + standard deviation across time)  \n",
    "- **Segment-level fully-connected layers**  \n",
    "- **Embedding and classification heads**\n",
    "\n",
    "#### Question 2.A (25 pts) â€” The TDNN Layer\n",
    "\n",
    "You will first implement a `TDNNLayer` module.\n",
    "\n",
    "- Input: a tensor of shape **[Batch, Time, Feature_dim]**  \n",
    "- Context: a set of time offsets (e.g. `{t-2, t-1, t, t+1, t+2}`)  \n",
    "- Output dimension: number of output units (e.g. 512)\n",
    "\n",
    "For each frame at time `t`, your layer should:\n",
    "1. Gather all context frames defined by the context offsets.  \n",
    "2. Concatenate them into a single feature vector.  \n",
    "3. Pass this vector through a `Linear` layer (and optionally a non-linearity).\n",
    "\n",
    "Because frames at the beginning and end lack full context, the output will have fewer time steps.  \n",
    "For a context `{t-2,â€¦,t+2}`, the resulting output shape should be **[Batch, Time âˆ’ 4, Output_dim]**.\n",
    "\n",
    "Your `TDNNLayer` class should implement:\n",
    "- `__init__(self, params)` â€” to define the context and linear layer  \n",
    "- `forward(self, x)` â€” to perform frame splicing and linear transformation\n",
    "\n",
    "Once implemented, verify that your `TDNNLayer` works correctly by:\n",
    "- Passing a sample from your **val loader** through the layer  \n",
    "- Checking that the output shape matches the expected dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd002442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNNLayer(nn.Module):\n",
    "    \"\"\"Time Delay Neural Network layer.\"\"\"\n",
    "    def __init__(self, input_dim, input_context, output_dim):\n",
    "        super().__init__()\n",
    "        assert isinstance(input_context, (list, tuple)) and len(input_context) > 0\n",
    "\n",
    "        self.context = list(input_context)\n",
    "        self.min_off = min(self.context)\n",
    "        self.max_off = max(self.context)\n",
    "\n",
    "        self.linear = nn.Linear(input_dim * len(self.context), output_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected [B,T,F], got {x.shape}\")\n",
    "\n",
    "        B, T, F = x.shape\n",
    "\n",
    "        # valid center times so all context frames exist\n",
    "        start_t = -self.min_off\n",
    "        end_t = T - self.max_off   # exclusive\n",
    "        if end_t <= start_t:\n",
    "            raise ValueError(f\"Sequence too short: T={T} for context [{self.min_off},{self.max_off}]\")\n",
    "\n",
    "        T_out = end_t - start_t\n",
    "\n",
    "        # splice: collect frames at offsets, then concat on feature dim\n",
    "        splices = []\n",
    "        for off in self.context:\n",
    "            splices.append(x[:, start_t + off : start_t + off + T_out, :])  # [B, T_out, F]\n",
    "\n",
    "        x_spliced = torch.cat(splices, dim=2)  # [B, T_out, F*len(context)]\n",
    "        y = self.linear(x_spliced)             # [B, T_out, output_dim]\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f8fdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([32, 498, 24])\n",
      "output shape: torch.Size([32, 494, 512])\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# from models import TDNNLayer\n",
    "\n",
    "model = TDNNLayer(24, [-2, -1, 0, 1, 2], 512)\n",
    "for data_point in val_loader:\n",
    "    melspec, spk_id, age, gender = data_point\n",
    "    print(f\"input shape: {melspec.shape}\")\n",
    "    output = model(melspec)\n",
    "    print(f\"output shape: {output.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5997f",
   "metadata": {},
   "source": [
    "#### Question 2.B (25 pts) â€” Full X-vector Architecture\n",
    "\n",
    "Next, you will build the complete **X-vector network** by combining multiple TDNN layers, a pooling layer, and fully-connected layers.\n",
    "\n",
    "#### Expected architecture\n",
    "Follow the structure described in the X-vector paper:\n",
    "\n",
    "1. Frame-level TDNN stack\n",
    "2. Statistics pooling  \n",
    "   - Compute the **mean** and **standard deviation** of the frame-level outputs across the time dimension.  \n",
    "   - Concatenate them â†’ shape becomes `[Batch, 3000]` for a 1500-dim input.\n",
    "3. Segment-level (utterance-level) layers\n",
    "   - Two fully-connected layers (e.g. 512 â†’ 512), followed by ReLU activations.  \n",
    "   - The output of the first segment-level layer (before activation) is the **embedding (x-vector)**.\n",
    "4. Classification head\n",
    "   - A final linear layer mapping the 512-dim embedding to the number of training speakers.  \n",
    "   - Used only during training (softmax + cross-entropy loss).\n",
    "\n",
    "Define a class `XVector(nn.Module)` that:\n",
    "- Inherits from `torch.nn.Module`  \n",
    "- Combines all components described above  \n",
    "- Returns: **Logits** for training\n",
    "\n",
    "Use a batch from your ```val loader``` to verify that:\n",
    "- The model runs without errors  \n",
    "- Output dimensions are as expected: `[Batch, Num_speakers]`  \n",
    "\n",
    "At the end of the lab, you are expected to save your networks in a ```models.py``` file, so they can be imported in future labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d473fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XVector(nn.Module):\n",
    "    def __init__(self, size=1, depth=1, num_speakers=1000, embedding_dim=512, input_dim=24):\n",
    "        super().__init__()\n",
    "        input_dim = 24  \n",
    "\n",
    "\n",
    "        self.tdnn1 = TDNNLayer(input_dim, [-2, -1, 0, 1, 2], 512)\n",
    "        self.tdnn2 = TDNNLayer(512, [-2, 0, 2], 512)\n",
    "        self.tdnn3 = TDNNLayer(512, [-3, 0, 3], 512)\n",
    "        self.tdnn4 = TDNNLayer(512, [0], 512)\n",
    "        self.tdnn5 = TDNNLayer(512, [0], 1500)\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.seg_fc1 = nn.Linear(3000, embedding_dim)\n",
    "        self.seg_fc2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_speakers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Frame-level\n",
    "        h = self.tdnn1(x)\n",
    "        h = self.tdnn2(h)\n",
    "        h = self.tdnn3(h)\n",
    "        h = self.tdnn4(h)\n",
    "        h = self.tdnn5(h)     # [B, T', 1500]\n",
    "\n",
    "        mean = h.mean(dim=1)\n",
    "        std = h.std(dim=1, unbiased=False)\n",
    "        stats = torch.cat([mean, std], dim=1)  # [B, 3000]\n",
    "\n",
    "        emb = self.seg_fc1(stats)\n",
    "        h2 = self.relu(emb)\n",
    "        h2 = self.relu(self.seg_fc2(h2))\n",
    "\n",
    "        logits = self.classifier(h2)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f546696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([32, 498, 24])\n",
      "output shape: torch.Size([32, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# from models import XVector\n",
    "\n",
    "model = XVector(input_dim=24, num_speakers=1000, embedding_dim=512)\n",
    "for data_point in val_loader:\n",
    "    melspec, spk_id, age, gender = data_point\n",
    "    print(f\"input shape: {melspec.shape}\")\n",
    "    output = model(melspec)\n",
    "    print(f\"output shape: {output.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376018b",
   "metadata": {},
   "source": [
    "#### Question 2.C (10 pts) â€” Embedding layer\n",
    "\n",
    "We now have a classical X-Vector system, that could be used to perform speaker identification on seen speakers. \n",
    "However, we can't perform speaker verification, on unseen speakers, as we would need to get the embeddings.\n",
    "Extend your `XVector` network from Question 2.B to optionally return the **embedding vector** used for speaker verification:\n",
    "\n",
    "- Modify the `forward()` function to include an optional argument `return_embedding` (default =`False`).\n",
    "- When `return_embedding=True`, the method should return the **512-dimensional embedding** produced by the first segment-level layer **before the classification head**.\n",
    "- When `return_embedding=False`, it should behave as before, returning the classification logits.\n",
    "\n",
    "To verify everything works, use a batch from your ```val loader``` with `return_embedding=True` and `return_embedding=False` to verify that:\n",
    "- `model(x, return_embedding=False)` returns logits of shape `[Batch_size, Num_speakers]`\n",
    "- `model(x, return_embedding=True)` returns embeddings of shape `[Batch_size, 512]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3027672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XVector(nn.Module):\n",
    "    def __init__(self, size=1, depth=1, num_speakers=1000, embedding_dim=512, input_dim=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tdnn1 = TDNNLayer(input_dim, [-2, -1, 0, 1, 2], 512)\n",
    "        self.tdnn2 = TDNNLayer(512, [-2, 0, 2], 512)\n",
    "        self.tdnn3 = TDNNLayer(512, [-3, 0, 3], 512)\n",
    "        self.tdnn4 = TDNNLayer(512, [0], 512)\n",
    "        self.tdnn5 = TDNNLayer(512, [0], 1500)\n",
    "\n",
    "        self.seg_fc1 = nn.Linear(3000, embedding_dim)  # embedding layer\n",
    "        self.seg_fc2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_speakers)\n",
    "\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        h = self.tdnn1(x)\n",
    "        h = self.tdnn2(h)\n",
    "        h = self.tdnn3(h)\n",
    "        h = self.tdnn4(h)\n",
    "        h = self.tdnn5(h)   # [B, T', 1500]\n",
    "\n",
    "        mean = h.mean(dim=1)\n",
    "        std = h.std(dim=1, unbiased=False)\n",
    "        stats = torch.cat([mean, std], dim=1)   # [B, 3000]\n",
    "\n",
    "        embedding = self.seg_fc1(stats)   \n",
    "\n",
    "        if return_embedding:\n",
    "            return embedding   # [B, 512]\n",
    "\n",
    "        h2 = self.relu(embedding)\n",
    "        h2 = self.relu(self.seg_fc2(h2))\n",
    "\n",
    "        logits = self.classifier(h2)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd84caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([32, 498, 24])\n",
      "logits shape: torch.Size([32, 1000])\n",
      "embedding shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# from models import XVector\n",
    "\n",
    "model = XVector(input_dim=24, num_speakers=1000, embedding_dim=512)\n",
    "for data_point in val_loader:\n",
    "    melspec, spk_id, age, gender = data_point\n",
    "    print(f\"input shape: {melspec.shape}\")\n",
    "    logits = model(melspec, return_embedding=False)\n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    embedding = model(melspec, return_embedding=True)\n",
    "    print(f\"embedding shape: {embedding.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71360d",
   "metadata": {},
   "source": [
    "### Question 3 (15 pts): Training and Validation\n",
    "\n",
    "In this question, you will train the X-vector network that you implemented in Question 2 on the training set. You will also evaluate its performance on the validation set using **classification accuracy** as the validation metric.\n",
    "\n",
    "1. **Training setup**\n",
    "   - Use your `train_loader` to iterate over the training data.\n",
    "   - Use **cross-entropy loss** (`nn.CrossEntropyLoss`) as the objective function.\n",
    "   - Use **Adam** an optimizer.\n",
    "   - Train the model for 10 epochs.\n",
    "\n",
    "2. **Validation**\n",
    "   - After each epoch, evaluate the model on the `val_loader`.\n",
    "   - Compute the **validation accuracy**\n",
    "   - Keep track of the **training loss**, **validation loss** and **validation accuracy** for each epoch.\n",
    "\n",
    "3. **Reporting**\n",
    "   - Print the training loss and validation accuracy across epochs.\n",
    "   - Identify the epoch that gives the **best validation accuracy**.\n",
    "\n",
    "After training:\n",
    "- Print the best validation accuracy achieved.\n",
    "\n",
    "\n",
    "_You can re-use your code from the last lab._\n",
    "\n",
    "_Also, if you want to make the training faster, you can reduce the number of speakers in you dataloaders (maybe 100?) and the internal dimensions of the network (512 -> 64)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc81d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers | train+val: 100 | train: 100 | val: 94 | test: 20 | total: 1094\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "\n",
    "from models import XVector\n",
    "from dataset import load_all_data\n",
    "\n",
    "number_train_speakers = 100\n",
    "#Dataset loading\n",
    "test_loader, val_loader, train_loader = load_all_data(\n",
    "                                                metadata_file='VoxCeleb2_AE/metadata_dev.csv', \n",
    "                                                data_directory='VoxCeleb2_AE/dev', \n",
    "                                                batch_size=128,\n",
    "                                                train_val_prop=0.9,\n",
    "                                                train_test_prop=0.95,\n",
    "                                                speaker_subset=[number_train_speakers,20]\n",
    "                                                )\n",
    "#New Model\n",
    "model = XVector(input_dim=24, num_speakers=number_train_speakers, embedding_dim=64, internal_dim=64)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-5)\n",
    "\n",
    "# Tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = -1.0\n",
    "best_epoch = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6d21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/10] | Train Loss: 6.2454, Acc: 4.95% | Val Loss: 5.2679, Acc: 0.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [02/10] | Train Loss: 4.0431, Acc: 8.22% | Val Loss: 5.5982, Acc: 0.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/10] | Train Loss: 3.7963, Acc: 11.35% | Val Loss: 6.7507, Acc: 0.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [04/10] | Train Loss: 3.5741, Acc: 14.10% | Val Loss: 7.8788, Acc: 4.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/10] | Train Loss: 3.4655, Acc: 16.00% | Val Loss: 7.3014, Acc: 3.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/10] | Train Loss: 3.2456, Acc: 19.12% | Val Loss: 10.2054, Acc: 4.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [07/10] | Train Loss: 3.0560, Acc: 22.21% | Val Loss: 10.4289, Acc: 3.75%\n"
     ]
    }
   ],
   "source": [
    "device='mps' # I'm using apple silicon, change for 'cuda' if you have a nvidia gpu / using colab, use 'cpu' if you are patient\n",
    "num_epochs=10\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    # --------------------\n",
    "    # ðŸ”¹ TRAINING PHASE\n",
    "    # --------------------\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for melspec, spk_id, age, gender in train_loader:\n",
    "        melspec = melspec.to(device)\n",
    "        spk_id = spk_id.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(melspec)\n",
    "        loss = criterion(logits, spk_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_correct += (preds == spk_id).sum().item()\n",
    "        train_total += spk_id.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "\n",
    "        \n",
    "\n",
    "    # --------------------\n",
    "    # ðŸ”¹ VALIDATION PHASE\n",
    "    # --------------------\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for melspec, spk_id, age, gender in tqdm(val_loader, desc=f\"Val {epoch+1}/{num_epochs}\", leave=False):\n",
    "            melspec = melspec.to(device)\n",
    "            spk_id = spk_id.to(device)\n",
    "\n",
    "            logits = model(melspec)\n",
    "            loss = criterion(logits, spk_id)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == spk_id).sum().item()\n",
    "            val_total += spk_id.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "    # --------------------\n",
    "    # ðŸ”¹ LOGGING\n",
    "    # --------------------\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1  \n",
    "        \n",
    "    print(f\"Epoch [{epoch+1:02d}/{num_epochs}] \"\n",
    "            f\"| Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% \"\n",
    "            f\"| Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Reporting\n",
    "print(\"\\nTraining loss across epochs:\")\n",
    "for i, tl in enumerate(train_losses, start=1):\n",
    "    print(f\"  Epoch {i:02d}: {tl:.4f}\")\n",
    "\n",
    "print(\"\\nValidation accuracy across epochs:\")\n",
    "for i, va in enumerate(val_accuracies, start=1):\n",
    "    print(f\"  Epoch {i:02d}: {va:.2f}%\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c1c73",
   "metadata": {},
   "source": [
    "## What did it learned?\n",
    "We saw that (hopefully) from the validation accuracy rising, your model learnt how to separate speakers.\n",
    "However, we have no metric (yet) that could help us characterize the exact efficiency of this model on unseen speakers... \n",
    "For this, we are going to use a set of visualization techniques.\n",
    "\n",
    "_Remark: You are not graded over the performances of your model, only the execution of the techniques. If it doesn't show what you epxected, don't re-train it 20 times._\n",
    "\n",
    "### Question 4.A (10pts): TSNE for speaker separation\n",
    "1. Extract all the speaker embeddings from the `test_loader`\n",
    "2. Use `sklearn.manifold.TSNE` function  to learn a 2D TSNE of the speakers embeddings\n",
    "3. Use `matplotlib.pyplot.scatter` function (or seaborn, if you like it pretty) to represent in a 2D figure the embedings from the test set with a different color per speaker.\n",
    "\n",
    "What can you observe?\n",
    "Are the embeddings separated?\n",
    "The t-SNE plot shows that some speakers form clear clusters. That means the model learned to group recordings from the same speaker close together in the embedding space. For example, the clusters on the far right and top-left are quite compact, which suggests the model can clearly recognize those speakers.\n",
    "\n",
    "However, in the middle of the plot, many points overlap and mix together. This means the model is not perfectly separating all speakers. Some voices are probably more similar to each other, or the model hasnâ€™t trained long enough to fully distinguish them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412383f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test embeddings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='mps'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_genders = []\n",
    "\n",
    "saved = {}\n",
    "\n",
    "def hook_fn(module, inp, out):\n",
    "    saved[\"emb\"] = out  \n",
    "\n",
    "handle = model.seg_fc2.register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for melspec, spk_id, age, gender in tqdm(test_loader, desc=\"Extract embeddings\"):\n",
    "        melspec = melspec.to(device)\n",
    "        spk_id = spk_id.to(device)\n",
    "\n",
    "        _ = model(melspec)          \n",
    "        emb = saved[\"emb\"]          \n",
    "        emb = torch.relu(emb)       \n",
    "\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "        all_labels.append(spk_id.cpu().numpy())\n",
    "        all_genders.append(gender.cpu().numpy() if torch.is_tensor(gender) else np.array(gender))\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "all_embeddings = np.concatenate(all_embeddings, axis=0)  # [N, embedding_dim]\n",
    "all_labels = np.concatenate(all_labels, axis=0)          # [N]\n",
    "all_genders = np.concatenate(all_genders, axis=0) if len(all_genders) else None\n",
    "\n",
    "print(\"Embeddings shape:\", all_embeddings.shape)\n",
    "print(\"Labels shape:\", all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ed4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\", random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)  \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=all_labels,      \n",
    "    s=12,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(\"t-SNE of X-Vector Speaker Embeddings (Test Set)\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.colorbar(scatter, label=\"Speaker ID\")\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE output shape:\", embeddings_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5ea24",
   "metadata": {},
   "source": [
    "### Question 4.B (5pts): Demographics details\n",
    "Modify your scatterplot to add a different marker per gender, do you see a natural separation of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for gender in np.unique(all_genders):\n",
    "    idx = all_genders == gender\n",
    "    \n",
    "    marker_style = \"o\" if gender == 0 else \"x\" \n",
    "    \n",
    "    scatter = plt.scatter(\n",
    "        embeddings_2d[idx, 0],\n",
    "        embeddings_2d[idx, 1],\n",
    "        c=all_labels[idx],      \n",
    "        cmap=\"viridis\",\n",
    "        marker=marker_style,\n",
    "        s=20,\n",
    "        alpha=0.8,\n",
    "        label=f\"Gender {gender}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"t-SNE of X-Vector Embeddings (Color=Speaker, Marker=Gender)\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.legend()\n",
    "plt.colorbar(scatter, label=\"Speaker ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620393a6",
   "metadata": {},
   "source": [
    "## Helper function: Saving and Loading your model\n",
    "If you want to save/load a version of your model, in case you need to restart the notebook, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243381e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_save_path = \"models/xvector_epoch8.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "import os\n",
    "from models import XVector\n",
    "model_save_path = \"models/xvector_epoch8.pt\"\n",
    "# model = XVector(input_dim=24, num_speakers=100, embedding_dim=64, )\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env]",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a1e3acc6f9f07695ebc7104267ab4dce9d872a6c86d133320855bdc2483d690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
